{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6292c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from f3 import init_event_model, load_weights_ckpt\n",
    "from f3.utils import setup_torch, plot_patched_features, BaseExtractor, ev_to_frames, smooth_time_weighted_rgb_encoding\n",
    "\n",
    "setup_torch(cudnn_benchmark=False)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(\n",
    "    context=\"talk\",\n",
    "    style=\"ticks\",\n",
    "    font_scale=1,\n",
    "    rc={\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.color\": \".8\",\n",
    "        \"grid.linewidth\": 0.75,\n",
    "        \"xtick.direction\": \"in\",\n",
    "        \"ytick.direction\": \"in\",\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.spines.top\": False,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Functions to handle event subsampling and feature similarity calculations.\n",
    "\"\"\"\n",
    "\n",
    "def subsample_events(events, factor):\n",
    "    \"\"\"\n",
    "        events: (N, 4)\n",
    "        factor: float\n",
    "    \"\"\"\n",
    "    num_events = events.shape[0]\n",
    "    indices = np.sort(rng.choice(num_events, size=int(num_events / factor), replace=False, shuffle=False))\n",
    "    # indices = np.linspace(0, num_events - 1, int(num_events / factor), dtype=int)\n",
    "    subsampled_events = events[indices]\n",
    "    # print(f\"Subsampled {num_events} events to {subsampled_events.shape[0]} events\")\n",
    "    return subsampled_events, subsampled_events.shape[0]\n",
    "\n",
    "def similarity(f1, f2):\n",
    "    \"\"\"\n",
    "        f1: (B, W, H, C)\n",
    "        f2: (B, W, H, C)\n",
    "    \"\"\"\n",
    "    C = f1.shape[-1]\n",
    "    f1 = f1.clone()[0].permute(2, 0, 1) # (C, W, H)\n",
    "    f2 = f2.clone()[0].permute(2, 0, 1) # (C, W, H)\n",
    "    f1 = f1.reshape(C, -1) # (C, W*H)\n",
    "    f2 = f2.reshape(C, -1) # (C, W*H)\n",
    "\n",
    "    B, C = f1.shape[:2]\n",
    "    normfactor = torch.cat([\n",
    "        f1.reshape(B, C, -1), f2.reshape(B, C, -1)\n",
    "    ], dim=-1).norm(dim=-1)[:, :, None, None] + 1e-7\n",
    "    f1 /= normfactor # (B, C, W, H)\n",
    "    f2 /= normfactor # (B, C, W, H)\n",
    "\n",
    "    # L2 distance between f1 and f2\n",
    "    # dist = torch.mean((f1 - f2) ** 2) / torch.mean(f1 ** 2)\n",
    "    # return dist.item()\n",
    "\n",
    "    # Cosine similarity between f1 and f2\n",
    "    sim = F.cosine_similarity(f1, f2, dim=0)\n",
    "    sim = torch.mean(sim)\n",
    "    return sim.item()\n",
    "\n",
    "    # Bhattacharyya distance between f1 and f2\n",
    "    # normalize f1 and f2 into probability distributions, where f can be negative\n",
    "    # f1 = f1.abs()\n",
    "    # f2 = f2.abs()\n",
    "    # f1 /= f1.sum(dim=0, keepdim=True)\n",
    "    # f2 /= f2.sum(dim=0, keepdim=True)\n",
    "    # bc = torch.sum(torch.sqrt(f1 * f2), dim=0)\n",
    "    # bc = -torch.log(bc + 1e-7)  # add small value to avoid log(0)\n",
    "    # return bc.mean().item()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"/home/richeek/GitHub/f3/outputs/patchff_fullcardaynightm3ed_small_20ms/models/config.yml\"\n",
    "ckpt = \"/home/richeek/GitHub/f3/outputs/patchff_fullcardaynightm3ed_small_20ms/models/last.pth\"\n",
    "# config = \"/home/richeek/GitHub/f3/outputs/patchff_fullcardaym3ed_small_20ms/models/config.yml\"\n",
    "# ckpt = \"/home/richeek/GitHub/f3/outputs/patchff_fullcardaym3ed_small_20ms/models/last.pth\"\n",
    "\n",
    "eff = init_event_model(config, return_feat=True).cuda()\n",
    "eff = torch.compile(eff)\n",
    "\n",
    "epoch, loss, acc = load_weights_ckpt(eff, ckpt, strict=False)\n",
    "\n",
    "print(f\"Loaded model from epoch {epoch} with loss {loss} and accuracy {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115af642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_day_city_hall_data/car_urban_day_city_hall_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_day_city_hall_data/50khz_car_urban_day_city_hall_data.npy\"\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_night_city_hall_data/car_urban_night_city_hall_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_night_city_hall_data/50khz_car_urban_night_city_hall_data.npy\"\n",
    "\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_day_rittenhouse_data/car_urban_day_rittenhouse_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_day_rittenhouse_data/50khz_car_urban_day_rittenhouse_data.npy\"\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_night_rittenhouse_data/car_urban_night_rittenhouse_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_night_rittenhouse_data/50khz_car_urban_night_rittenhouse_data.npy\"\n",
    "\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_day_penno_small_loop_data/car_urban_day_penno_small_loop_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_day_penno_small_loop_data/50khz_car_urban_day_penno_small_loop_data.npy\"\n",
    "\n",
    "h5path = \"/home/richeek/GitHub/f3/data/car_urban_day_ucity_small_loop_data/car_urban_day_ucity_small_loop_data.h5\"\n",
    "tspath = \"/home/richeek/GitHub/f3/data/car_urban_day_ucity_small_loop_data/50khz_car_urban_day_ucity_small_loop_data.npy\"\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_night_ucity_small_loop_data/car_urban_night_ucity_small_loop_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_night_ucity_small_loop_data/50khz_car_urban_night_ucity_small_loop_data.npy\"\n",
    "\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_day_schuylkill_tunnel_data/car_urban_day_schuylkill_tunnel_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_day_schuylkill_tunnel_data/50khz_car_urban_day_schuylkill_tunnel_data.npy\"\n",
    "\n",
    "h5file = h5py.File(h5path, 'r')\n",
    "tsfile = np.load(tspath, allow_pickle=True)[()]\n",
    "\n",
    "extractor = BaseExtractor(h5path, tspath, w=1280, h=720, time_ctx=20000,\n",
    "                          time_pred=20000, bucket=1000, max_numevents_ctx=2000000,\n",
    "                          randomize_ctx=False, camera=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62603c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = int(240 * 1e6) # in us\n",
    "time = int(130 * 1e6) # in us\n",
    "img_size = (1280, 720)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ctx, totcnt = extractor.get_ctx_fixedtime(time)\n",
    "\n",
    "event_frame = ev_to_frames(ctx, torch.tensor([totcnt], dtype=torch.int32), *img_size)\n",
    "axes[0].imshow(event_frame[0].T, cmap='gray')\n",
    "axes[0].set_title(f'Original Events ({totcnt})')\n",
    "axes[0].axis('off')\n",
    "\n",
    "sub_events_2, sub_counts_2 = subsample_events(ctx, 2)\n",
    "event_frame_2 = ev_to_frames(sub_events_2, torch.tensor([sub_counts_2], dtype=torch.int32), *img_size)\n",
    "axes[1].imshow(event_frame_2[0].T, cmap='gray')\n",
    "axes[1].set_title(f'Subsampled by 2 ({sub_counts_2})')\n",
    "axes[1].axis('off')\n",
    "\n",
    "sub_events_10, sub_counts_10 = subsample_events(ctx, 10)\n",
    "event_frame_10 = ev_to_frames(sub_events_10, torch.tensor([sub_counts_10], dtype=torch.int32), *img_size)\n",
    "axes[2].imshow(event_frame_10[0].T, cmap='gray')\n",
    "axes[2].set_title(f'Subsampled by 10 ({sub_counts_10})')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "def ret_pca(context, count):\n",
    "    ctx_tensor = context.cuda()\n",
    "    cnt_tensor = torch.tensor([count], dtype=torch.int32).cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feats = eff(ctx_tensor, cnt_tensor)[1].clone()\n",
    "    pca, _ = plot_patched_features(feats[0].permute(1, 0, 2), plot=False)\n",
    "    return pca\n",
    "\n",
    "pca = ret_pca(ctx, totcnt)\n",
    "axes[0].imshow(pca)\n",
    "axes[0].set_title(f'Original Events Features ({totcnt})')\n",
    "axes[0].axis('off')\n",
    "\n",
    "pca_2 = ret_pca(sub_events_2, sub_counts_2)\n",
    "axes[1].imshow(pca_2)\n",
    "axes[1].set_title(f'Subsampled by 2 Features ({sub_counts_2})')\n",
    "axes[1].axis('off')\n",
    "\n",
    "pca_10 = ret_pca(sub_events_10, sub_counts_10)\n",
    "axes[2].imshow(pca_10)\n",
    "axes[2].set_title(f'Subsampled by 10 Features ({sub_counts_10})')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75031a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = (640, 0, 1280, 640)  # (x1, y1, x2, y2)\n",
    "event_frame_cropped = event_frame[0, crop[0]:crop[2], crop[1]:crop[3]]\n",
    "event_frame_10_cropped = event_frame_10[0, crop[0]:crop[2], crop[1]:crop[3]]\n",
    "plt.imshow(event_frame.T, cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(event_frame_cropped.T, cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(event_frame_10_cropped.T, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "pca_cropped = pca[crop[1]:crop[3], crop[0]:crop[2]]\n",
    "pca_10_cropped = pca_2[crop[1]:crop[3], crop[0]:crop[2]]\n",
    "plt.imshow(pca)\n",
    "plt.show()\n",
    "plt.imshow(pca_cropped)\n",
    "plt.show()\n",
    "plt.imshow(pca_10_cropped)\n",
    "plt.show()\n",
    "\n",
    "cv2.imwrite(\"/home/richeek/GitHub/f3/outputs/matching/event_subsampling.jpg\", event_frame.T.numpy())\n",
    "cv2.imwrite(\"/home/richeek/GitHub/f3/outputs/matching/event_subsampling_10.jpg\", event_frame_10.T.numpy())\n",
    "cv2.imwrite(\"/home/richeek/GitHub/f3/outputs/matching/event_subsampling_cropped.jpg\", event_frame_cropped.T.numpy())\n",
    "cv2.imwrite(\"/home/richeek/GitHub/f3/outputs/matching/event_subsampling_cropped_10.jpg\", event_frame_10_cropped.T.numpy())\n",
    "\n",
    "cv2.imwrite(\"/home/richeek/GitHub/f3/outputs/matching/event_subsampling_features.jpg\",\n",
    "            cv2.cvtColor(pca, cv2.COLOR_BGR2RGB))\n",
    "cv2.imwrite(\"/home/richeek/GitHub/f3/outputs/matching/event_subsampling_features_10.jpg\",\n",
    "            cv2.cvtColor(pca_2, cv2.COLOR_BGR2RGB))\n",
    "cv2.imwrite(\"/home/richeek/GitHub/f3/outputs/matching/event_subsampling_features_cropped.jpg\",\n",
    "            cv2.cvtColor(pca_cropped, cv2.COLOR_BGR2RGB))\n",
    "cv2.imwrite(\"/home/richeek/GitHub/f3/outputs/matching/event_subsampling_features_cropped_10.jpg\",\n",
    "            cv2.cvtColor(pca_10_cropped, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd70300",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "time_ctx = 20000 # in us\n",
    "last_event_time = extractor.events_t[-1] # in us\n",
    "least_event_count = 400000\n",
    "\n",
    "# Extract dataset name from h5path\n",
    "dataset_name = h5path.split('/')[-2]  # Get the folder name which contains the dataset name\n",
    "\n",
    "# Initialize or load existing similarity_tensors dictionary\n",
    "try:\n",
    "    similarity_tensors\n",
    "except NameError:\n",
    "    similarity_tensors = {}\n",
    "\n",
    "similarity_tensors[dataset_name] = []\n",
    "\n",
    "subsample_fracs = np.arange(1, 22, 1)\n",
    "\n",
    "for t0 in tqdm(np.arange(time_ctx, last_event_time, 1e6)):\n",
    "    ctx, totcnt = extractor.get_ctx_fixedtime(int(t0))\n",
    "    if totcnt < least_event_count: continue\n",
    "    with torch.no_grad():\n",
    "        feat_0 = eff(ctx.cuda(), torch.tensor([totcnt], dtype=torch.int32).cuda())[1]\n",
    "    similarity_tensors[dataset_name].append(torch.zeros(subsample_fracs.size, dtype=torch.float32))\n",
    "    for i, frac in enumerate(subsample_fracs):\n",
    "        sub_events, sub_counts = subsample_events(ctx, frac)\n",
    "        with torch.no_grad():\n",
    "            feat = eff(sub_events.cuda(), torch.tensor([sub_counts], dtype=torch.int32).cuda())[1]\n",
    "        dist = similarity(feat_0, feat)\n",
    "        # print(f\"Distance between {i} subsampled events and original events: {dist}\")\n",
    "        similarity_tensors[dataset_name][ctr][i] = dist\n",
    "    ctr += 1\n",
    "\n",
    "similarity_tensors[dataset_name] = torch.stack(similarity_tensors[dataset_name], dim=0)\n",
    "similarity_tensors[dataset_name] = similarity_tensors[dataset_name].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c15c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "# matplotlib.rcParams['text.usetex'] = True\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Use seaborn color palette\n",
    "colors = sns.color_palette(\"husl\", len(similarity_tensors))\n",
    "\n",
    "for i, (dataset_name, similarity_data) in enumerate(similarity_tensors.items()):\n",
    "    similarity_mean = np.mean(similarity_data, axis=0)\n",
    "    similarity_std = np.std(similarity_data, axis=0)\n",
    "    \n",
    "    color = colors[i]\n",
    "    \n",
    "    plt.plot(subsample_fracs, similarity_mean, linewidth=2, color=color, \n",
    "             label=dataset_name.replace('_', ' ').title(), alpha=0.8)\n",
    "    plt.fill_between(subsample_fracs, similarity_mean - similarity_std, \n",
    "                     similarity_mean + similarity_std, color=color, alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"Subsampling Factor\", fontsize=14)\n",
    "plt.ylabel(\"Cosine Similarity\", fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.title('Feature similarity with event sub-sampling', fontsize=16, pad=10)\n",
    "plt.xticks(np.arange(1, 22, 2), fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80290355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the similarity tensors\n",
    "output_dir = \"/home/richeek/GitHub/f3/outputs/matching/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"similarity_tensors.npy\")\n",
    "np.save(output_path, similarity_tensors)\n",
    "print(f\"Saved similarity tensors to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13966bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "# matplotlib.rcParams['text.usetex'] = True\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "output_path = \"/home/richeek/GitHub/f3/outputs/matching/similarity_tensors.npy\"\n",
    "similarity_tensors_saved = np.load(output_path, allow_pickle=True).item()\n",
    "plotlist = [\n",
    "    \"car_urban_day_ucity_small_loop_data\",\n",
    "    \"car_urban_night_ucity_small_loop_data\",\n",
    "    \"car_urban_day_rittenhouse_data\",\n",
    "    \"car_urban_night_rittenhouse_data\",\n",
    "]\n",
    "\n",
    "# try:\n",
    "#     subsample_fracs\n",
    "# except NameError:\n",
    "#     subsample_fracs = np.arange(1, 21, 1)\n",
    "subsample_fracs = np.arange(1, 21, 1)\n",
    "\n",
    "# colors = sns.color_palette(\"husl\")\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Custom colors\n",
    "markers = ['o', 's', '^', 'D']\n",
    "\n",
    "for idx, dataset_name in enumerate(plotlist):\n",
    "    similarity_data = similarity_tensors_saved[dataset_name][:, :len(subsample_fracs)]\n",
    "    similarity_mean = np.mean(similarity_data, axis=0)\n",
    "    similarity_std = np.std(similarity_data, axis=0)\n",
    "    \n",
    "    color = colors[idx]\n",
    "    marker = markers[idx]\n",
    "    \n",
    "    plt.plot(subsample_fracs, similarity_mean, linewidth=3, label=dataset_name.replace('_', ' ').replace(\" data\", \"\").replace(\"car urban\", \"\").title(), \n",
    "             color=color, marker=marker, markersize=6, markevery=2)\n",
    "    plt.fill_between(subsample_fracs, similarity_mean - similarity_std, \n",
    "                     similarity_mean + similarity_std, color=color, alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"Sub-sampling Factor\", fontsize=20)\n",
    "plt.ylabel(\"Cosine Similarity\", fontsize=20)\n",
    "plt.grid(True, alpha=0.3)\n",
    "# plt.legend(loc='upper right', fontsize=20)\n",
    "# plt.title('Feature similarity with event sub-sampling', fontsize=24, pad=15)\n",
    "plt.xticks([1, 4, 8, 12, 16, 20], fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(output_dir, \"feature_similarity_with_subsampling.pdf\"))\n",
    "plt.savefig(\"feature_similarity_with_subsampling.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20335ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots showing cosine similarity and optical flow AEE\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 7))\n",
    "\n",
    "# Left subplot: Cosine Similarity (from previous cell)\n",
    "plotlist = [\n",
    "    \"car_urban_day_ucity_small_loop_data\",\n",
    "    \"car_urban_night_ucity_small_loop_data\", \n",
    "    \"car_urban_day_rittenhouse_data\",\n",
    "    \"car_urban_night_rittenhouse_data\",\n",
    "]\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "markers = ['o', 's', '^', 'D']\n",
    "labels = ['Day Ucity Small Loop', 'Night Ucity Small Loop', 'Day Rittenhouse', 'Night Rittenhouse']\n",
    "\n",
    "for idx, dataset_name in enumerate(plotlist):\n",
    "    similarity_data = similarity_tensors_saved[dataset_name][:, :len(subsample_fracs)]\n",
    "    similarity_mean = np.mean(similarity_data, axis=0)\n",
    "    similarity_std = np.std(similarity_data, axis=0)\n",
    "    \n",
    "    color = colors[idx]\n",
    "    marker = markers[idx]\n",
    "    label = labels[idx]\n",
    "    \n",
    "    ax1.plot(subsample_fracs, similarity_mean, linewidth=3, \n",
    "             label=label, color=color, marker=marker, markersize=6, markevery=2)\n",
    "    ax1.fill_between(subsample_fracs, similarity_mean - similarity_std, \n",
    "                     similarity_mean + similarity_std, color=color, alpha=0.2)\n",
    "\n",
    "ax1.set_xlabel(\"Sub-sampling Factor\", fontsize=20)\n",
    "ax1.set_ylabel(\"Cosine Similarity\", fontsize=20)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks([1, 4, 8, 12, 16, 20])\n",
    "ax1.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "# Right subplot: Optical Flow AEE\n",
    "subsampling_factors = [1, 2, 5, 10]\n",
    "day_ucity_small_loop = [5.41, 5.82, 7.12, 9.11]\n",
    "night_ucity_small_loop = [6.97, 7.79, 9.85, 12.83]\n",
    "day_rittenhouse = [5.31, 5.78, 6.57, 8.23]\n",
    "night_rittenhouse = [6.15, 7.77, 10.08, 12.14]\n",
    "\n",
    "ax2.plot(subsampling_factors, day_ucity_small_loop, \n",
    "         linewidth=3, marker=markers[0], color=colors[0], label=labels[0])\n",
    "ax2.plot(subsampling_factors, night_ucity_small_loop, \n",
    "         linewidth=3, marker=markers[1], color=colors[1], label=labels[1])\n",
    "ax2.plot(subsampling_factors, day_rittenhouse, \n",
    "         linewidth=3, marker=markers[2], color=colors[2], label=labels[2])\n",
    "ax2.plot(subsampling_factors, night_rittenhouse, \n",
    "         linewidth=3, marker=markers[3], color=colors[3], label=labels[3])\n",
    "\n",
    "ax2.set_xlabel('Subsampling Factor', fontsize=20)\n",
    "ax2.set_ylabel('Average Endpoint Error (AEE)', fontsize=20)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(subsampling_factors)\n",
    "ax2.set_yticks([4, 6, 8, 10, 12])\n",
    "ax2.tick_params(axis='both', which='major', labelsize=16)\n",
    "ax2.set_ylim(4, 13)\n",
    "\n",
    "# Create a single legend for both subplots\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', \n",
    "           ncol=2, fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.15)  # Make room for the legend\n",
    "plt.savefig('combined_subsampling_analysis.svg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d2e9d",
   "metadata": {},
   "source": [
    "## Nighttime Qualitative results on Flow and Depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a348d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from f3.tasks.optical_flow.utils import flow_viz_np, EventFFFlow\n",
    "\n",
    "\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_night_city_hall_data/car_urban_night_city_hall_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_night_city_hall_data/50khz_car_urban_night_city_hall_data.npy\"\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_night_rittenhouse_data/car_urban_night_rittenhouse_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_night_rittenhouse_data/50khz_car_urban_night_rittenhouse_data.npy\"\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_night_penno_small_loop_darker_data/car_urban_night_penno_small_loop_darker_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_night_penno_small_loop_darker_data/50khz_car_urban_night_penno_small_loop_darker_data.npy\"\n",
    "\n",
    "# SPOT\n",
    "h5path = \"/home/richeek/GitHub/f3/data/spot_indoor_building_loop/spot_indoor_building_loop_data.h5\"\n",
    "tspath = \"/home/richeek/GitHub/f3/data/spot_indoor_building_loop/50khz_spot_indoor_building_loop_data.npy\"\n",
    "\n",
    "\n",
    "h5file = h5py.File(h5path, 'r')\n",
    "tsfile = np.load(tspath, allow_pickle=True)[()]\n",
    "\n",
    "extractor = BaseExtractor(h5path, tspath, w=1280, h=720, time_ctx=20000,\n",
    "                          time_pred=20000, bucket=1000, max_numevents_ctx=2000000,\n",
    "                          randomize_ctx=False, camera=\"left\")\n",
    "\n",
    "config = \"/home/richeek/GitHub/f3/outputs/optical_flow/optflow_trainm3ed_20msff_pyr5_28k/models/flow_config.yaml\"\n",
    "ckpt_path = \"/home/richeek/GitHub/f3/outputs/optical_flow/optflow_trainm3ed_20msff_pyr5_28k/models/last.pth\"\n",
    "\n",
    "flow_yaml = yaml.safe_load(open(config, 'r'))\n",
    "flow_model = EventFFFlow(flow_yaml[\"eventff\"], flowhead_config=flow_yaml[\"flowhead\"])\n",
    "flow_model.eventff = torch.compile(flow_model.eventff)\n",
    "flow_model.flowhead = torch.compile(flow_model.flowhead)\n",
    "\n",
    "ckpt = torch.load(ckpt_path)\n",
    "flow_model.load_state_dict(ckpt[\"model\"])\n",
    "epoch, loss = ckpt[\"epoch\"], ckpt[\"loss\"]\n",
    "del ckpt\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Loaded optical flow model from {ckpt_path}. Epoch: {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_idx = 500\n",
    "time = h5file[\"ovc/ts\"][rgb_idx] # in us\n",
    "\n",
    "rgb = h5file[\"ovc/rgb/data\"][rgb_idx]\n",
    "plt.imshow(rgb)\n",
    "plt.show()\n",
    "\n",
    "ctx, totcnt = extractor.get_ctx_fixedtime(time)\n",
    "eframe = ev_to_frames(ctx, torch.tensor([totcnt], dtype=torch.int32), *img_size)\n",
    "plt.imshow(eframe[0].T, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    flow, _ = flow_model(ctx.cuda(), torch.tensor([totcnt], dtype=torch.int32).cuda())\n",
    "flow_rgb = flow_viz_np(flow[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.imshow(flow_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf21904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colormaps as cm\n",
    "from f3.tasks.depth.utils import EventFFDepthAnythingV2, get_disparity_image\n",
    "\n",
    "config = {\n",
    "    \"dav2_config\": {\"size\": 518, \"encoder\": \"vitb\"},\n",
    "    \"eventff\": \"/home/richeek/GitHub/f3/outputs/patchff_fullcardaym3ed_small_20ms/models/config.yml\",\n",
    "}\n",
    "ckpt_path = \"/home/richeek/GitHub/f3/outputs/monoculardepth/dav2b_fullm3ed_pseudo_518x518x20/models/best.pth\"\n",
    "\n",
    "depth_model = EventFFDepthAnythingV2(config[\"eventff\"], config[\"dav2_config\"])\n",
    "depth_model.eventff = torch.compile(depth_model.eventff)\n",
    "\n",
    "last_dict = torch.load(ckpt_path, weights_only=True)\n",
    "depth_model.load_state_dict(last_dict[\"model\"], strict=False)\n",
    "epoch, results = last_dict[\"epoch\"], last_dict[\"results\"]\n",
    "del last_dict\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Loaded depth model from {ckpt_path}. Epoch: {epoch}, Results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_idx = 500\n",
    "time = h5file[\"ovc/ts\"][rgb_idx] # in us\n",
    "cmap = cm.get_cmap(\"magma\")\n",
    "\n",
    "rgb = h5file[\"ovc/rgb/data\"][rgb_idx]\n",
    "plt.imshow(rgb)\n",
    "plt.show()\n",
    "\n",
    "ctx, totcnt = extractor.get_ctx_fixedtime(time)\n",
    "eframe = ev_to_frames(ctx, torch.tensor([totcnt], dtype=torch.int32), *img_size)\n",
    "plt.imshow(eframe[0].T, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "depth_pred, ff = depth_model.infer_image(ctx.cuda(), torch.tensor([totcnt], dtype=torch.int32).cuda())\n",
    "depth_image = get_disparity_image(depth_pred, torch.ones_like(depth_pred, dtype=bool), cmap=cmap)\n",
    "plt.imshow(depth_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e635844",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('depth_flow_night_video_penno_small_loop_darker.mp4', fourcc, 25.0, (1920, 1080))\n",
    "down_size = (1920 // 2, 1080 // 2)\n",
    "\n",
    "for rgb_idx in tqdm(range(0, len(h5file[\"ovc/rgb/data\"]))):\n",
    "    time = h5file[\"ovc/ts\"][rgb_idx]  # in us\n",
    "    if time <= 20000: continue  # skip first 20ms\n",
    "\n",
    "    rgb = h5file[\"ovc/rgb/data\"][rgb_idx]\n",
    "    ctx, totcnt = extractor.get_ctx_fixedtime(time)\n",
    "\n",
    "    if totcnt < 50000: continue  # skip frames with less than 100k events\n",
    "    eframe = ev_to_frames(ctx, torch.tensor([totcnt], dtype=torch.int32), *img_size)[0].T.cpu().numpy()[..., None].repeat(3, axis=-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        flow, _ = flow_model(ctx.cuda(), torch.tensor([totcnt], dtype=torch.int32).cuda())\n",
    "        depth_pred, ff = depth_model.infer_image(ctx.cuda(), torch.tensor([totcnt], dtype=torch.int32).cuda())\n",
    "    flow_rgb = flow_viz_np(flow[0].permute(1, 2, 0).cpu().numpy()) * (eframe == 255)\n",
    "    depth_rgb = get_disparity_image(depth_pred, torch.ones_like(depth_pred, dtype=bool), cmap=cmap)\n",
    "\n",
    "    # place on 4 quadrants\n",
    "    combined = np.zeros((down_size[1] * 2, down_size[0] * 2, 3), dtype=np.uint8)\n",
    "    combined[:down_size[1], :down_size[0]] = cv2.resize(rgb, down_size)\n",
    "    combined[:down_size[1], down_size[0]:] = cv2.resize(eframe, down_size)\n",
    "    combined[down_size[1]:, :down_size[0]] = cv2.resize(flow_rgb, down_size)\n",
    "    combined[down_size[1]:, down_size[0]:] = cv2.resize(depth_rgb, down_size)\n",
    "    combined = cv2.cvtColor(combined, cv2.COLOR_RGB2BGR)\n",
    "    out.write(combined)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c40040e",
   "metadata": {},
   "source": [
    "# Denoising by thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"/home/richeek/GitHub/f3/outputs/patchff_fullcardaym3ed_small_20ms/models/config.yml\"\n",
    "ckpt = \"/home/richeek/GitHub/f3/outputs/patchff_fullcardaym3ed_small_20ms/models/last.pth\"\n",
    "\n",
    "# config = \"/home/richeek/GitHub/f3/outputs/patchff_fullcardaynightm3ed_small_20ms/models/config.yml\"\n",
    "# ckpt = \"/home/richeek/GitHub/f3/outputs/patchff_fullcardaynightm3ed_small_20ms/models/last.pth\"\n",
    "\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_day_city_hall_data/car_urban_day_city_hall_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_day_city_hall_data/50khz_car_urban_day_city_hall_data.npy\"\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_night_city_hall_data/car_urban_night_city_hall_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_night_city_hall_data/50khz_car_urban_night_city_hall_data.npy\"\n",
    "\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_day_rittenhouse_data/car_urban_day_rittenhouse_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_day_rittenhouse_data/50khz_car_urban_day_rittenhouse_data.npy\"\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_night_rittenhouse_data/car_urban_night_rittenhouse_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_night_rittenhouse_data/50khz_car_urban_night_rittenhouse_data.npy\"\n",
    "\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_day_penno_small_loop_data/car_urban_day_penno_small_loop_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_day_penno_small_loop_data/50khz_car_urban_day_penno_small_loop_data.npy\"\n",
    "\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_day_ucity_small_loop_data/car_urban_day_ucity_small_loop_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_day_ucity_small_loop_data/50khz_car_urban_day_ucity_small_loop_data.npy\"\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_night_ucity_small_loop_data/car_urban_night_ucity_small_loop_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_night_ucity_small_loop_data/50khz_car_urban_night_ucity_small_loop_data.npy\"\n",
    "\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/car_urban_day_schuylkill_tunnel_data/car_urban_day_schuylkill_tunnel_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/car_urban_day_schuylkill_tunnel_data/50khz_car_urban_day_schuylkill_tunnel_data.npy\"\n",
    "\n",
    "# SPOT\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/spot_indoor_building_loop/spot_indoor_building_loop_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/spot_indoor_building_loop/50khz_spot_indoor_building_loop_data.npy\"\n",
    "h5path = \"/home/richeek/GitHub/f3/data/spot_indoor_obstacles/spot_indoor_obstacles_data.h5\"\n",
    "tspath = \"/home/richeek/GitHub/f3/data/spot_indoor_obstacles/50khz_spot_indoor_obstacles_data.npy\"\n",
    "# h5path = \"/home/richeek/GitHub/f3/data/spot_indoor_stairwell/spot_indoor_stairwell_data.h5\"\n",
    "# tspath = \"/home/richeek/GitHub/f3/data/spot_indoor_stairwell/50khz_spot_indoor_stairwell_data.npy\"\n",
    "\n",
    "h5file = h5py.File(h5path, 'r')\n",
    "tsfile = np.load(tspath, allow_pickle=True)[()]\n",
    "\n",
    "extractor = BaseExtractor(h5path, tspath, w=1280, h=720, time_ctx=20000,\n",
    "                          time_pred=20000, bucket=1000, max_numevents_ctx=2000000,\n",
    "                          randomize_ctx=False, camera=\"left\")\n",
    "\n",
    "eff = init_event_model(config, return_feat=True).cuda()\n",
    "eff = torch.compile(eff)\n",
    "\n",
    "epoch, loss, acc = load_weights_ckpt(eff, ckpt, strict=False)\n",
    "\n",
    "print(f\"Loaded model from epoch {epoch} with loss {loss} and accuracy {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import time as time_module\n",
    "\n",
    "start_idx = 200\n",
    "end_idx = len(h5file[\"ovc/rgb/data\"])  # You can adjust this to a smaller range if needed\n",
    "step = 10\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "for rgb_idx in range(start_idx, end_idx, step):\n",
    "    clear_output(wait=True)\n",
    "    timestamp = h5file[\"ovc/ts\"][rgb_idx]  # in us\n",
    "    rgb = h5file[\"ovc/rgb/data\"][rgb_idx]\n",
    "    ctx, totcnt = extractor.get_ctx_fixedtime(timestamp)\n",
    "    eframe = ev_to_frames(ctx, torch.tensor([totcnt], dtype=torch.int32), *img_size)\n",
    "    \n",
    "    ctx_tensor = ctx.cuda()\n",
    "    cnt_tensor = torch.tensor([totcnt], dtype=torch.int32).cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, feats = eff(ctx_tensor, cnt_tensor)\n",
    "    \n",
    "    # # Get PCA visualization of features\n",
    "    # pca, _ = plot_patched_features(feats[0].permute(1, 0, 2), plot=False)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     flow, _ = flow_model(ctx_tensor, cnt_tensor)\n",
    "    # flow_rgb = flow_viz_np(flow[0].permute(1, 2, 0).cpu().numpy()) * (eframe[0].T.cpu().numpy() == 255)[..., None]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        depth_pred, ff = depth_model.infer_image(ctx_tensor, cnt_tensor)\n",
    "    depth_image = get_disparity_image(depth_pred, torch.ones_like(depth_pred, dtype=bool), cmap=cm.get_cmap(\"magma\"))\n",
    "    \n",
    "    predframe = smooth_time_weighted_rgb_encoding(torch.sigmoid(logits).cpu().numpy() > 0.6225)\n",
    "    \n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(rgb[40:-40,:,::-1])\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"RGB Frame (Index: {rgb_idx})\")\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.title(f\"Event Frame (Events: {totcnt})\")\n",
    "    plt.imshow(eframe[0].T, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # plt.subplot(2, 2, 3)\n",
    "    # plt.title(\"PCA of Features\")\n",
    "    # plt.imshow(pca)\n",
    "    # plt.axis('off')\n",
    "\n",
    "    # plt.subplot(2, 2, 3)\n",
    "    # plt.title(\"Optical Flow\")\n",
    "    # plt.imshow(flow_rgb)\n",
    "    # plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.title(\"Depth Prediction\")\n",
    "    plt.imshow(depth_image)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.title(\"Predicted Frame\")\n",
    "    plt.imshow(predframe[0].transpose(1, 0, 2))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Timestamp: {timestamp/1e6:.2f} seconds\", fontsize=16)\n",
    "    display(plt.gcf())\n",
    "    \n",
    "    time_module.sleep(0.04)\n",
    "    plt.close()\n",
    "print(\"Animation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prolev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
