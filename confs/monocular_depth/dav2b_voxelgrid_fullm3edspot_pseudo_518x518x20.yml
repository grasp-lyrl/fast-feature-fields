# Training options
lr: 6.0e-6 # Learning rate
lr_end_factor: 1.0 # Learning rate decay
clip_grad: 0 # Gradient clipping
epochs: 100 # Number of epochs
log_interval: 10 # Log interval in epochs
val_interval: 10 # Validation interval in epochs
polarity: [False, False] # Use polarity for the training

train:
  num_workers: 4 # Number of workers for the dataloader
  batch: 8 # Batch size of event blocks tuned to a 24GB GPU
  mini_batch: 8 # Mini batch size of events
  datasets:
  - confs/monocular_depth/full_spot_pseudo_train.yml

val:
  num_workers: 4 # Number of workers for the dataloader
  batch: 1 # Batch size of event blocks
  mini_batch: 1 # Mini batch size of events 
  datasets:
  - confs/monocular_depth/full_spot_pseudo_val.yml

randomize_ctx: True
min_numevents_ctx: 100000 # Number of events to use for the context
max_numevents_ctx: 800000 # Number of events to use for the context
time_ctx: 20000 # Time to use for the context in us (where to subsample the numevents_ctx events from)
bucket: 1000 # Number of us in each frame
frame_sizes: [1280, 720] # Size of the frame

# Loss options
loss: ssimae # Loss function
scales: 4 # Number of scales for the gradient matching loss
alpha: 0.3 # Weight for the gradient matching loss
max_disparity: 1000 # Maximum disparity for the depth map

# dav2 config to load
dav2_config:
  size: 518
  encoder: vitb
  ckpt: src/prolev/tasks/depth/checkpoints/depth_anything_v2_vitb.pth

eventmodel: "voxelgrid"